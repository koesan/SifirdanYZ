{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN93tDxDR0Qo"
      },
      "source": [
        "Harika bir seçim\\! Generative Adversarial Networks (GAN), yapay zekanın en büyüleyici konularından biridir. İstediğin gibi, kodun içine boğulmadan önce **GAN'ın mantığını** çok basit bir hikaye ile anlatalım, sonra da bu hikayeyi koda dökelim.\n",
        "\n",
        "Aşağıda, bir Jupyter Notebook gibi adım adım çalıştırabileceğin, açıklamalı ve sonunda eğitim sürecinin GIF'ini oluşturan kodu hazırladım.\n",
        "\n",
        "-----\n",
        "\n",
        "### 1\\. GAN Nedir? (Kalpazan ve Dedektif Hikayesi)\n",
        "\n",
        "GAN, aslında birbiriyile yarışan iki ayrı yapay zeka modelidir.\n",
        "\n",
        "1.  **Generator (Üretici - \"Kalpazan\"):**\n",
        "      * **Görevi:** Hiç yoktan (rastgele sayılardan) gerçekçi sahte paralar (bizim durumumuzda el yazısı rakamlar) üretmek.\n",
        "      * **Amacı:** Dedektifi kandırmak. \"Bak bu gerçek\\!\" dedirtmek.\n",
        "2.  **Discriminator (Ayırt Edici - \"Dedektif\"):**\n",
        "      * **Görevi:** Eline gelen resmin \"Gerçek veri setinden mi\" yoksa \"Kalpazanın ürettiği sahte mi\" olduğunu anlamak.\n",
        "      * **Amacı:** Asla kandırılmamak.\n",
        "\n",
        "[Image of GAN architecture diagram]\n",
        "\n",
        "**Eğitim Süreci Şöyle İşler:**\n",
        "\n",
        "  * Kalpazan ilk başta berbat resimler çizer (karıncalı ekran gibi).\n",
        "  * Dedektif buna bakar ve \"Bu sahte\\!\" der.\n",
        "  * Kalpazan, \"Neyi yanlış yaptım?\" diye bakar ve kendini geliştirir.\n",
        "  * Dedektif de sürekli gerçek resimler görerek kendini geliştirir.\n",
        "  * Bu döngü binlerce kez tekrarlandığında, Kalpazan o kadar iyi sahteler üretir ki, Dedektif artık ayırt edemez hale gelir. İşte o an GAN eğitilmiş demektir.\n",
        "\n",
        "-----\n",
        "\n",
        "### 2\\. Kodlama Aşaması\n",
        "\n",
        "Bu kodu bir Jupyter Notebook hücresi gibi veya `.py` dosyası olarak çalıştırabilirsin.\n",
        "\n",
        "#### Adım 1: Kütüphaneler ve Veri Hazırlığı\n",
        "\n",
        "Veriyi **-1 ile 1 arasına** sıkıştırıyoruz. Bu GAN'lar için çok önemlidir çünkü `tanh` aktivasyon fonksiyonunu kullanacağız."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NNapG2WeR0Qs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Veri hazır! Görüntü boyutu: (60000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# 1. Veri Setini Yükle ve Hazırla\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Görüntüleri [28, 28, 1] boyutuna getir (Siyah beyaz kanalını ekle)\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "# Pikselleri -1 ile 1 arasına normalize et (GAN standardı)\n",
        "# Normalde pikseller 0-255 arasındadır.\n",
        "train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "# Veriyi karıştır ve batch'lere böl\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "print(\"Veri hazır! Görüntü boyutu:\", train_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_Glb9kaR0Qt"
      },
      "source": [
        "#### Adım 2: Modelleri Oluşturma (Vanilla GAN)\n",
        "\n",
        "Burada Convolution (CNN) yerine `Dense` (Tam Bağlantılı) katmanlar kullanarak en temel \"Vanilla\" yapısını kuruyoruz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rejS4YivR0Qt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modeller oluşturuldu.\n"
          ]
        }
      ],
      "source": [
        "# --- GENERATOR (ÜRETİCİ / KALPAZAN) ---\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # Giriş: Rastgele gürültü vektörü (seed). Boyutu 100 olsun.\n",
        "    # Bu gürültüyü alıp 7*7*256 boyutunda bir veriye dönüştüreceğiz.\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU()) # ReLU yerine LeakyReLU GAN'da daha iyi çalışır\n",
        "\n",
        "    # Şeklini bir görüntü taslağına benzetelim\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "\n",
        "    # Upsampling (Görüntüyü büyütme) - 7x7 -> 14x14\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # 14x14 -> 28x28 (Final boyutu)\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Çıktı katmanı: 28x28x1 boyutunda görüntü.\n",
        "    # Aktivasyon 'tanh' çünkü veriyi -1 ile 1 arasına sıkıştırmıştık.\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- DISCRIMINATOR (AYIRT EDİCİ / DEDEKTİF) ---\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # Giriş: 28x28x1 boyutunda bir resim\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dropout(0.3)) # Ezberlemeyi önlemek için\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten()) # Düzleştir\n",
        "    model.add(layers.Dense(1))  # Çıktı: Tek bir sayı (Gerçek olma ihtimali)\n",
        "\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "print(\"Modeller oluşturuldu.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQvRdtzR0Qu"
      },
      "source": [
        "#### Adım 3: Kayıp Fonksiyonu ve Optimizasyon\n",
        "\n",
        "Dedektif ve Kalpazan'ın ne kadar başarılı olduğunu nasıl ölçeceğiz?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ekkiOiplR0Qu"
      },
      "outputs": [],
      "source": [
        "# Binary Crossentropy: İki sınıflı (Gerçek/Sahte) sınıflandırma için standart.\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# --- Discriminator Kaybı ---\n",
        "# Dedektif şunları yapmalı:\n",
        "# 1. Gerçek resimlere \"1\" (Gerçek) demeli.\n",
        "# 2. Sahte resimlere \"0\" (Sahte) demeli.\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output) # Gerçekleri 1 bildi mi?\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # Sahteleri 0 bildi mi?\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "# --- Generator Kaybı ---\n",
        "# Kalpazan şunu yapmalı:\n",
        "# Ürettiği sahte resimlere Dedektifin \"1\" (Gerçek) demesini sağlamalı.\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "# Optimizasyoncular (Adam optimizer standarttır)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9m6oTOiR0Qu"
      },
      "source": [
        "#### Adım 4: Eğitim Döngüsü (En Kritik Yer)\n",
        "\n",
        "Burası sihrin gerçekleştiği yer. Her adımda:\n",
        "\n",
        "1.  Generator bir resim üretir.\n",
        "2.  Discriminator hem gerçek hem sahte resme bakar.\n",
        "3.  İkisi de hatalarına göre güncellenir.\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wsJ55cJNR0Qv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eğitim başlıyor... Lütfen bekleyin (GPU yoksa biraz sürebilir)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 04:03:17.193067: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 tamamlandı.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 04:05:52.209858: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 tamamlandı.\n",
            "Epoch 3 tamamlandı.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 04:10:49.840860: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 tamamlandı.\n",
            "Epoch 5 tamamlandı.\n",
            "Epoch 6 tamamlandı.\n",
            "Epoch 7 tamamlandı.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-24 04:20:51.332746: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 tamamlandı.\n",
            "Epoch 9 tamamlandı.\n",
            "Epoch 10 tamamlandı.\n",
            "Epoch 11 tamamlandı.\n",
            "Epoch 12 tamamlandı.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m image_batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Her epoch sonunda ilerlemeyi kaydet\u001b[39;00m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tamamlandı.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 50 # Eğitim tur sayısı\n",
        "noise_dim = 100 # Rastgele gürültü boyutu\n",
        "num_examples_to_generate = 16 # GIF için her seferinde 16 örnek kaydedelim\n",
        "\n",
        "# Sabit bir gürültü (seed) oluşturuyoruz ki eğitimin gelişimini hep aynı \"tohumdan\" görelim.\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
        "\n",
        "@tf.function # Bu dekoratör işlemi hızlandırır\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # 1. Üretici sahte resim üretir\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        # 2. Dedektif hem gerçek hem sahte resimleri değerlendirir\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        # 3. Kayıplar hesaplanır\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # 4. Gradyanlar (değişim yönü) hesaplanır\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    # 5. Modeller güncellenir (Öğrenme gerçekleşir)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "# Görselleştirme Fonksiyonu\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    # Eğitim değil (training=False), sadece çıktı alıyoruz\n",
        "    predictions = model(test_input, training=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "    for i in range(predictions.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        # Görüntüyü -1,1 aralığından 0,255 aralığına çekip gösteriyoruz\n",
        "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "    plt.close() # Belleği şişirmemek için kapat\n",
        "    # (Eğer notebook'ta anlık görmek istersen plt.show() diyebilirsin ama GIF için savefig şart)\n",
        "\n",
        "# --- EĞİTİMİ BAŞLAT ---\n",
        "print(\"Eğitim başlıyor... Lütfen bekleyin (GPU yoksa biraz sürebilir)\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for image_batch in train_dataset:\n",
        "        train_step(image_batch)\n",
        "\n",
        "    # Her epoch sonunda ilerlemeyi kaydet\n",
        "    print(f\"Epoch {epoch + 1} tamamlandı.\")\n",
        "    generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "print(\"Eğitim Tamamlandı!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37JVzpGR0Qv"
      },
      "source": [
        "#### Adım 5: GIF Oluşturma\n",
        "\n",
        "Eğitim bittiğinde klasörde bir sürü `image_at_epoch_xxxx.png` olacak. Bunları birleştirip süreci izleyelim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IukmaVfR0Qv"
      },
      "outputs": [],
      "source": [
        "anim_file = 'gan_egitimi.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "    filenames = glob.glob('image_at_epoch*.png')\n",
        "    filenames = sorted(filenames)\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "\n",
        "print(f\"GIF oluşturuldu: {anim_file}\")\n",
        "\n",
        "# Sonuçlardan birini ekrana bas\n",
        "import IPython\n",
        "if os.path.exists(anim_file):\n",
        "    print(\"GIF hazırlanıyor...\")\n",
        "    # Colab veya Jupyter kullanıyorsan:\n",
        "    # IPython.display.Image(open(anim_file,'rb').read())\n",
        "else:\n",
        "    print(\"GIF dosyası bulunamadı.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHXhla86R0Qw"
      },
      "source": [
        "-----\n",
        "\n",
        "### Çıktıda Ne Göreceksin?\n",
        "\n",
        "1.  **İlk Epochlar:** Gri, anlamsız gürültü kareleri. (Kalpazan henüz işi bilmiyor).\n",
        "2.  **Orta Epochlar:** Mürekkep lekeleri gibi şekiller, yavaş yavaş rakama benzeyen karaltılar.\n",
        "3.  **Son Epochlar:** 0'dan 9'a kadar belirgin, el yazısı rakamlar.\n",
        "\n",
        "Bu kodda `make_generator_model` içinde `Conv2DTranspose` kullandım. Eğer \"en basit Vanilla GAN\" (sadece Dense katmanlar) istersen, Convolution katmanlarını silip sadece `Dense` katmanlar koyabiliriz ama o zaman görüntüler çok daha bulanık olur. Bu haliyle (DCGAN türevi) sonuçlar çok daha tatmin edici olacaktır.\n",
        "\n",
        "**Bir sonraki adım:** Bu kodu Google Colab'de \"GPU\" seçeneğini açarak çalıştırmanı öneririm, CPU'da 50 epoch biraz uzun sürebilir. Yardıma ihtiyacın olursa buradayım\\!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.12.0",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
