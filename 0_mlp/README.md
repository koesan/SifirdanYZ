## ğŸ¬ CanlÄ± EÄŸitim SimÃ¼lasyonu

3 epoch boyunca adÄ±m adÄ±m **ileri besleme** ve **geri yayÄ±lÄ±m** sÃ¼recini gÃ¶steren animasyon:

![EÄŸitim SimÃ¼lasyonu](./image/training_simulation.gif)


# ğŸ§  NÃ¶ral AÄŸ Nedir?


![NÃ¶ral AÄŸ GÃ¶rseli](./image/neural_network.png)


NÃ¶ral aÄŸ, bilgisayarÄ±n verilerden Ã¶ÄŸrenmesini saÄŸlayan bir sistemdir.

Ä°Ã§inde birÃ§ok kÃ¼Ã§Ã¼k yapay "nÃ¶ron" bulunur. Her nÃ¶ron birkaÃ§ sayÄ± alÄ±r (girdi), bunlarÄ± iÅŸler ve bir sonuÃ§ (Ã§Ä±ktÄ±) Ã¼retir.

---

## âš™ï¸ NÃ¶ron NasÄ±l Ã‡alÄ±ÅŸÄ±r?

Ã–nce tÃ¼m girdiler kendi **aÄŸÄ±rlÄ±klarÄ±yla (w)** Ã§arpÄ±lÄ±r, sonra bu deÄŸerler toplanÄ±r ve kÃ¼Ã§Ã¼k bir **bias (b)** eklenir:

$$
n = (x_1 \cdot w_1) + (x_2 \cdot w_2) + b
$$

Bu iÅŸlem sonucu Ã§Ä±kan deÄŸer bir **aktivasyon fonksiyonuna** gÃ¶nderilir. Aktivasyon fonksiyonu, bu sonucu iÅŸler ve nÃ¶ronun Ã§Ä±ktÄ±sÄ±nÄ± Ã¼retir.

---

## â• Bias (b) Nedir?

Bias, nÃ¶ronun iÃ§ine eklenen kÃ¼Ã§Ã¼k bir sabit deÄŸerdir. Bu deÄŸer, nÃ¶ronun Ã§Ä±ktÄ±sÄ±nÄ± yukarÄ± veya aÅŸaÄŸÄ± kaydÄ±rarak daha esnek Ã¶ÄŸrenmesini saÄŸlar.

TÃ¼m girdiler sÄ±fÄ±r olsa bile, bias sayesinde nÃ¶ron kÃ¼Ã§Ã¼k bir Ã§Ä±kÄ±ÅŸ Ã¼retebilir.

---

## ğŸ¯ Aktivasyon Fonksiyonu Nedir?

Aktivasyon fonksiyonu, nÃ¶ronun Ã¼rettiÄŸi sonucu **yorumlayan ve dÃ¶nÃ¼ÅŸtÃ¼ren** bir mekanizmadÄ±r.

Bir anlamda nÃ¶ronun "karar verme noktasÄ±"dÄ±r. Hangi bilginin iletileceÄŸini, hangisinin bastÄ±rÄ±lacaÄŸÄ±nÄ± belirler. Bu sayede aÄŸ, sadece basit doÄŸrusal iliÅŸkilerle sÄ±nÄ±rlÄ± kalmaz; **karmaÅŸÄ±k, eÄŸrisel ve Ã§ok katmanlÄ± desenleri** de Ã¶ÄŸrenebilir.

KÄ±saca, aktivasyon fonksiyonu bir nÃ¶ronu "hesap makinesinden" Ã§Ä±karÄ±p **Ã¶ÄŸrenebilen birime** dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

FarklÄ± aktivasyon fonksiyonlarÄ± farklÄ± davranÄ±ÅŸlar saÄŸlar:

* **ReLU (Rectified Linear Unit):** Negatif deÄŸerleri sÄ±fÄ±rlar, pozitifleri olduÄŸu gibi geÃ§irir. Basit ve hÄ±zlÄ±dÄ±r.
* **tanh:** SonuÃ§larÄ± -1 ile 1 arasÄ±na sÄ±kÄ±ÅŸtÄ±rÄ±r, daha dengeli bir Ã¶ÄŸrenme saÄŸlar.

---

**TÃ¼revin formÃ¼lÃ¼:**

$$
\mathbf{f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}}
$$

---

TÃ¼rev alÄ±rken seÃ§tiÄŸimiz deÄŸiÅŸken, **deÄŸiÅŸtirmek istediÄŸimiz girdidir.**

**Ã–rneÄŸin**

```python
# Matematiksel olarak limit almasanÄ±z bile, h deÄŸerini bu kadar kÃ¼Ã§Ã¼k 
# seÃ§erek pratik olarak tÃ¼rev deÄŸerini hesaplamÄ±ÅŸ olursunuz.

h = 0.0001
# inputs
a = 2.0
b = -3.0  # <--- DeÄŸiÅŸken (Girdi)
c = 10.0
d1 = a*b + c

b += h    # <--- Sadece b'yi deÄŸiÅŸtiriyoruz.
d2 = a*b + c
print('slope', (d2 - d1)/h)
```

Kodda, $d$ Ã§Ä±ktÄ±sÄ±ndaki deÄŸiÅŸimin $b$'deki kÃ¼Ã§Ã¼k bir deÄŸiÅŸimden $\left(\frac{d_2 - d_1}{h}\right)$ kaynaklandÄ±ÄŸÄ±nÄ± Ã¶lÃ§tÃ¼ÄŸÃ¼nÃ¼z iÃ§in, doÄŸal olarak **$b$'ye gÃ¶re tÃ¼rev** almanÄ±z gerekir.

### $\mathbf{d}$'nin $\mathbf{b}$'ye GÃ¶re TÃ¼revi ($\mathbf{\frac{dd}{db}}$)

Fonksiyonumuz: $d = a \cdot b + c$

---

### 1. AdÄ±m: TÃ¼rev OperatÃ¶rÃ¼nÃ¼ Uygulama

TÃ¼m ifadeye $b$'ye gÃ¶re tÃ¼rev alma komutunu ($\frac{d}{db}$) uygularÄ±z. Toplama kuralÄ± gereÄŸi, her terimin tÃ¼revi ayrÄ± ayrÄ± alÄ±nÄ±r.

$$
\frac{d}{db}(d) = \frac{d}{db} (a \cdot b) + \frac{d}{db} (c)
$$

---

### 2. AdÄ±m: Sabit Terimin TÃ¼revi

$c$ deÄŸiÅŸkeni, tÃ¼rev alÄ±nan deÄŸiÅŸkene ($b$) baÄŸlÄ± olmadÄ±ÄŸÄ± iÃ§in **sabittir**. Bir sabitin tÃ¼revi daima sÄ±fÄ±rdÄ±r.

$$
\frac{d}{db} (c) = 0
$$

---

### 3. AdÄ±m: KatsayÄ±lÄ± DeÄŸiÅŸkenin TÃ¼revi

$a \cdot b$ teriminde, $a$ bir **sabit katsayÄ±dÄ±r**. $b$'nin $b$'ye gÃ¶re tÃ¼revi ise $1$'dir. KatsayÄ± olduÄŸu gibi kalÄ±r ve deÄŸiÅŸkenin tÃ¼revi ile Ã§arpÄ±lÄ±r.

$$
\frac{d}{db} (a \cdot b) = a \cdot \underbrace{\frac{d}{db} (b)}_{1} = a
$$

---

### 4. AdÄ±m: Sonucu BirleÅŸtirme

BulduÄŸumuz sonuÃ§larÄ± topladÄ±ÄŸÄ±mÄ±zda, $d$'nin $b$'ye gÃ¶re tÃ¼revi ($\frac{dd}{db}$) elde edilir.

$$
\frac{dd}{db} = a + 0 = a
$$

**KÄ±saca Ã‡Ä±karÄ±m:** $d$ Ã§Ä±ktÄ±sÄ±ndaki deÄŸiÅŸim, $b$ girdisindeki deÄŸiÅŸimin **tam $a$ katÄ±dÄ±r**. Yapay zeka bu $a$ deÄŸerini (gradyanÄ±) kullanarak aÄŸÄ±rlÄ±klarÄ± ayarlar.

AynÄ± ÅŸekilde $b$ yerine $a$'yÄ± deÄŸiÅŸtirseydik, bu sefer $d$'nin $a$'ya gÃ¶re tÃ¼revini almÄ±ÅŸ olurduk. Bu durumda da $d$'nin $a$'ya gÃ¶re tÃ¼revi bize $b$'yi verirdi.

---

### $\mathbf{d}$'nin $\mathbf{c}$'ye GÃ¶re TÃ¼revi ($\mathbf{\frac{dd}{dc}}$)

Bu sefer $\frac{d}{dc}$ komutunu uyguluyoruz; yani $\mathbf{c}$ deÄŸiÅŸkendir, $a$ ve $b$ sabittir.

$\frac{dd}{dc} = \frac{d}{dc} (a \cdot b + c)$

1. **Sabit Ã‡arpÄ±mÄ±nÄ±n TÃ¼revi:** $a \cdot b$ terimi, tÃ¼rev aldÄ±ÄŸÄ±mÄ±z $c$ deÄŸiÅŸkenine baÄŸlÄ± deÄŸildir. DolayÄ±sÄ±yla **tamamÄ± sabittir** ve sabitin tÃ¼revi $\mathbf{0}$'dÄ±r.
   
   $\frac{d}{dc} (a \cdot b) = \mathbf{0}$

2. **DeÄŸiÅŸkenin TÃ¼revi:** $c$'nin $c$'ye gÃ¶re tÃ¼revi ($\frac{d}{dc}(c)$) her zaman $\mathbf{1}$'dir.
   
   $\frac{d}{dc} (c) = \mathbf{1}$

### SonuÃ§

TÃ¼revleri topladÄ±ÄŸÄ±mÄ±zda: $\frac{dd}{dc} = 0 + 1 = \mathbf{1}$

**KÄ±saca Ã‡Ä±karÄ±m:**

Bu, $c$'yi (bias/sapma) bir birim artÄ±rÄ±rsanÄ±z, $d$ Ã§Ä±ktÄ±sÄ±nÄ±n da **tam olarak bir birim** artacaÄŸÄ±nÄ± gÃ¶sterir. EÄŸimin $\mathbf{1}$ olmasÄ±, $c$'nin $d$ Ã¼zerindeki etkisinin bire bir olduÄŸunu ifade eder.

**Yapay Zeka BaÄŸlamÄ±:** $c$ bir **sapma (bias)** terimini temsil eder. SapmanÄ±n gradyanÄ± her zaman $1$'dir, Ã§Ã¼nkÃ¼ $c$'deki deÄŸiÅŸim Ã§Ä±ktÄ±yÄ± aynÄ± miktarda deÄŸiÅŸtirir.

**Ã–rneÄŸin:**

```python
h = 0.0001
# inputs
a = 2.0
b = -3.0  
c = 10.0 # <--- DeÄŸiÅŸken (Girdi)
d1 = a*b + c

c += h    # <--- Sadece b'yi deÄŸiÅŸtiriyoruz.

# c nin artmasÄ± d2 nin sonucunu artÄ±rÄ±r.
d2 = a*b + c
print('slope', (d2 - d1)/h)

# Ã§Ä±ktÄ±
slope 0.9999999999976694
```

> **NOT:**
> 
> $d$'nin $c$'ye gÃ¶re tÃ¼revini alÄ±rken, Python'dan `0.9999999999976694` gibi bir sonuÃ§ almanÄ±zÄ±n nedeni **kayan nokta (floating-point) hassasiyetinden kaynaklanan bir hatadÄ±r**.  Bilgisayarlar, ondalÄ±klÄ± sayÄ±larÄ± (kayan noktalarÄ±) depolarken ve Ã§ok kÃ¼Ã§Ã¼k $h$ ile yaklaÅŸÄ±k hesaplama yaparken **her zaman minik yuvarlama hatalarÄ±** yapar.

---

Bir sinir aÄŸÄ±nÄ±n nasÄ±l Ã¶ÄŸrendiÄŸini anlamanÄ±n temeli, aslÄ±nda Ã§ok basit bir matematik fikrine dayanÄ±r:

$\frac{f(x+h) - f(x)}{h}$

Yani **bir fonksiyonun eÄŸimini (deÄŸiÅŸim hÄ±zÄ±nÄ±)** bulmak.

Bu mantÄ±ÄŸÄ± sinir aÄŸlarÄ±na uyarladÄ±ÄŸÄ±mÄ±zda, Ã¶rneÄŸin

$\frac{dd}{db} = a \quad \text{ve} \quad \frac{dd}{dc} = 1$

gibi tÃ¼revler karÅŸÄ±mÄ±za Ã§Ä±kar. Bu sayÄ±lar, modelin â€œhangi aÄŸÄ±rlÄ±ÄŸÄ± ne kadar deÄŸiÅŸtirmesi gerektiÄŸiniâ€ sÃ¶yler. Yapay zeka'nÄ±n Ã¶ÄŸrenme sÃ¼reci aslÄ±nda bundan ibarettir:

Tahmin hatasÄ±nÄ± Ã¶lÃ§en bÃ¼yÃ¼k bir fonksiyon vardÄ±r ve aÄŸ, bu hatayÄ± **gradyanÄ± (eÄŸimi)** takip ederek azaltmaya Ã§alÄ±ÅŸÄ±r. Yani bu tÃ¼revler, aÄŸÄ±n geri yayÄ±lÄ±m (backpropagation) sÄ±rasÄ±nda kullandÄ±ÄŸÄ± **yÃ¶n tabelalarÄ±** gibidir.

â€œBu aÄŸÄ±rlÄ±ÄŸÄ± biraz azalt, ÅŸu sapmayÄ± biraz artÄ±râ€ derler.

GÃ¶rÃ¼nÃ¼ÅŸte karmaÅŸÄ±k olan sinir aÄŸlarÄ±nÄ±n temelinde aslÄ±nda **tÃ¼rev almak ve eÄŸimlere gÃ¶re ayar yapmak** yatar.

---

### Geri YayÄ±lÄ±m (Backpropagation) Nedir, Neden ve NasÄ±l?

Geri YayÄ±lÄ±m (Backpropagation), sinir aÄŸlarÄ±nÄ±n **Ã¶ÄŸrenmesini saÄŸlayan** temel algoritmadÄ±r.

| **Soru**           | **Cevap (KÄ±sa ve Basit)**                                                                                                                                                                                                | **micrograd'daki KarÅŸÄ±lÄ±ÄŸÄ±**                                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| **Nedir?**         | En sonda yapÄ±lan **hata hesaplamasÄ±nÄ±** alÄ±p, bu hatanÄ±n tÃ¼m aÄŸ boyunca **geriye doÄŸru** yayÄ±lmasÄ±dÄ±r.                                                                                                                   | OluÅŸturduÄŸumuz $\mathbf{Value}$ nesneleri zincirini (grafiÄŸi) tersten takip etme.                                            |
| **Neden YapÄ±lÄ±r?** | Hata en son Ã§Ä±ktÄ±da oluÅŸur, ancak hataya sebep olanlar ilk katmanlardaki **aÄŸÄ±rlÄ±klar**dÄ±r. Geri YayÄ±lÄ±m, her bir aÄŸÄ±rlÄ±ÄŸÄ±n ($a$) hataya ne kadar katkÄ±da bulunduÄŸunu (gradyanÄ±nÄ±) bulmak iÃ§in yapÄ±lÄ±r.                  | AÄŸÄ±rlÄ±klarÄ±n ve sapmalarÄ±n gradyanÄ±nÄ± ($\mathbf{\frac{dd}{da}}$, $\mathbf{\frac{dd}{dc}}$) bulmak.                           |
| **NasÄ±l YapÄ±lÄ±r?** | **Zincir KuralÄ± (Chain Rule)** kullanÄ±larak yapÄ±lÄ±r. En sondaki hatadan baÅŸlanÄ±r, her bir iÅŸlemde **yerel tÃ¼rev** (o anki iÅŸlemin tÃ¼revi) hesaplanÄ±r ve bu tÃ¼rev, arkadan gelen tÃ¼revle Ã§arpÄ±larak geriye doÄŸru yayÄ±lÄ±r. | Her $\mathbf{Value}$ nesnesindeki `_backward()` fonksiyonunu, grafik sÄ±rasÄ±nÄ±n (topolojik sÄ±ralama) **tersinden** Ã§aÄŸÄ±rarak. |
| **AmacÄ±?**         | Her aÄŸÄ±rlÄ±ÄŸÄ±n gradyanÄ± bulunduktan sonra, **aÄŸÄ±rlÄ±klar gradyanÄ±n tersi yÃ¶nde** ayarlanÄ±r (Gradient Descent). Bu, hatayÄ± minimuma indirmektir.                                                                            | Ã–ÄŸrenmek, yani modeli hatasÄ±z hale getirmek.                                                                                 |

**KÄ±saca:** Geri YayÄ±lÄ±m, bir sinir aÄŸÄ±nÄ±n yaptÄ±ÄŸÄ± hatayÄ± bulduktan sonra, bu hatayÄ± oluÅŸturan **aÄŸÄ±rlÄ±klarÄ±n suÃ§unu (gradyanÄ±nÄ±)** bulmak iÃ§in **zincirleme tÃ¼rev alma** iÅŸlemidir. Bu sayede, aÄŸÄ±rlÄ±klarÄ± dÃ¼zelterek aÄŸÄ±n bir sonraki tahminde daha iyi olmasÄ±nÄ± saÄŸlarÄ±z.

### Geri YayÄ±lÄ±m (Backpropagation) BaÅŸlangÄ±cÄ±

Sinir aÄŸlarÄ±nÄ±n Ã¶ÄŸrenme sÃ¼recinin kritik aÅŸamasÄ± olan **Geri YayÄ±lÄ±m (Backpropagation)**, sistemdeki hatanÄ±n kaynaklarÄ±nÄ± geriye doÄŸru tespit etmeye odaklanÄ±r.

#### 1. BaÅŸlangÄ±Ã§ Durumu: ($\mathbf{grad = 0}$)

Modelin ileri besleme (forward pass) aÅŸamasÄ± tamamlandÄ±ÄŸÄ±nda, henÃ¼z herhangi bir hata (Loss) hesaplanmamÄ±ÅŸ ve bu hata geriye doÄŸru yayÄ±lmaya baÅŸlamamÄ±ÅŸtÄ±r. Bu nedenle, sisteme dahil olan tÃ¼m deÄŸiÅŸkenlerin hataya katkÄ±sÄ± (gradyanÄ±) baÅŸlangÄ±Ã§ta **mantÄ±ksal olarak 0'dÄ±r.**


![](./image/output1.png)


###### 2. Geri YayÄ±lÄ±ma BaÅŸlama NoktasÄ±: ($\mathbf{L}$'nin $\mathbf{L}$'ye GÃ¶re TÃ¼revi)

Geri yayÄ±lÄ±mÄ±n kalbi **Zincir KuralÄ±**dÄ±r. Bu kural, hatayÄ± en sondan alÄ±p en baÅŸtaki aÄŸÄ±rlÄ±klara doÄŸru geri iletmemizi saÄŸlar.

**HatÄ±rlayalÄ±m:** Zincir KuralÄ±, uzun bir iÅŸlem zincirinde deÄŸiÅŸimi bulmak iÃ§in aradaki tÃ¼m yerel tÃ¼revleri Ã§arpmamÄ±zÄ± sÃ¶yler:

$$
\frac{d c}{d a}=\frac{d c}{d b}\times\frac{d b}{d a}
$$

Geri yayÄ±lÄ±mÄ± baÅŸlatmak iÃ§in zincirin en sonunda, yani **Nihai Hata ($\mathbf{L}$)** noktasÄ±nda bir baÅŸlangÄ±Ã§ deÄŸeri tanÄ±mlamamÄ±z gerekir.

- **Basit TÃ¼rev KuralÄ±:** Bir deÄŸiÅŸkenin kendisine gÃ¶re tÃ¼revi daima $\mathbf{1}$'dir.

$$
\frac{d L}{d L}=1
$$

Daha Ã¶nce gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z gibi, bir deÄŸiÅŸkenin kendisine gÃ¶re tÃ¼revi daima $\mathbf{1}$'dir. Yani $\mathbf{L}$'nin kendisindeki bir birimlik deÄŸiÅŸim, $\mathbf{L}$'yi tam olarak bir birim deÄŸiÅŸtirir. Bu $\mathbf{1}$ sayÄ±sÄ±, Geri YayÄ±lÄ±m zincirine verdiÄŸimiz **ilk ve en Ã¶nemli itme gÃ¼cÃ¼dÃ¼r**. Bu gÃ¼cÃ¼ alÄ±p, Zincir KuralÄ± gereÄŸi geriye doÄŸru tÃ¼m $\mathbf{Value}$ nesnelerinin yerel tÃ¼revleriyle Ã§arpmaya baÅŸlarÄ±z.


![](./image/output2.png)


### 3. SorumluluÄŸu DaÄŸÄ±tma: $\mathbf{L}$'nin $\mathbf{d}$ ve $\mathbf{f}$ Ä°le TÃ¼revlerini Bulma

Geri YayÄ±lÄ±mda baÅŸlangÄ±Ã§ komutunu ($\mathbf{L.grad = 1}$) verdikten sonra, ÅŸimdi bu sorumluluÄŸu $\mathbf{L}$'yi oluÅŸturan Ã¶nceki dÃ¼ÄŸÃ¼mlere ($\mathbf{d}$ ve $\mathbf{f}$) daÄŸÄ±tmamÄ±z gerekiyor.

**UnutmayalÄ±m:** $\mathbf{L}$'nin $\mathbf{d}$ ve $\mathbf{f}$ ile iliÅŸkisi Ã§arpma iÅŸlemidir: $\mathbf{L = d \cdot f}$.

**A)** $\mathbf{L}$'nin $\mathbf{d}$'ye GÃ¶re TÃ¼revi:

| **AdÄ±m**             | **Ä°ÅŸlem (Limit TanÄ±mÄ±)**                      | **AÃ§Ä±klama (Neden Bu DeÄŸiÅŸimi YaptÄ±k?)**                                                                                                                                                          |
| -------------------- | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **BaÅŸlangÄ±Ã§**        | $\frac{f(x+h) - f(x)}{h}$ formÃ¼lÃ¼             | **Genel Kural:** DeÄŸiÅŸim hÄ±zÄ±nÄ± bulmak iÃ§in $\mathbf{x}$ yerine $\mathbf{d}$'yi, $\mathbf{f(x)}$ yerine nihai sonuÃ§ $\mathbf{L}$'yi koyuyoruz.                                                    |
| **DeÄŸiÅŸim Uygulama** | $\frac{(d+h) \cdot f - d \cdot f}{h}$         | **Kurala Uyum:** $\mathbf{L = d \cdot f}$ olduÄŸu iÃ§in, $\mathbf{d}$'yi $\mathbf{h}$ kadar artÄ±rÄ±p ($\mathbf{d+h}$ yaparak) Ã§Ä±kan yeni $\mathbf{L}$ sonucunu hesapladÄ±k ve eski sonuÃ§tan Ã§Ä±kardÄ±k. |
| **Parantezi AÃ§ma**   | $\frac{d \cdot f + h \cdot f - d \cdot f}{h}$ | Ã‡arpmayÄ± daÄŸÄ±tÄ±p, hangi terimlerin birbirini gÃ¶tÃ¼receÄŸini gÃ¶rme aÅŸamasÄ±.                                                                                                                          |
| **SadeleÅŸtirme**     | $\frac{h \cdot f}{h}$                         | $\mathbf{d \cdot f}$ terimleri birbirini sÄ±fÄ±rladÄ±. Geriye, $\mathbf{h}$ deÄŸiÅŸikliÄŸinden kaynaklanan **saf deÄŸiÅŸim** kaldÄ±.                                                                       |
| **Nihai SonuÃ§**      | $\mathbf{f}$                                  | $\mathbf{h}$'leri sadeleÅŸtirdik. Kalan $\mathbf{f}$, $\mathbf{d}$'deki deÄŸiÅŸimin $\mathbf{L}$'yi tam olarak $\mathbf{f}$ kadar etkilediÄŸini gÃ¶steren **hÄ±z katsayÄ±mÄ±zdÄ±r**.                       |

**B)** $\mathbf{L}$'nin $\mathbf{f}$'ye GÃ¶re TÃ¼revi

Bu sefer $\mathbf{f}$'nin $\mathbf{L}$'ye olan yerel sorumluluÄŸunu buluyoruz. Bu, ilk durumun simetriÄŸidir. Yani SonuÃ§ **d** dir.

Ã–zetle, $\mathbf{L = d \cdot f}$ Ã§arpma iÅŸlemi iÃ§in, $\mathbf{d}$'nin gradyanÄ± ($\frac{dL}{dd}$) direkt olarak $\mathbf{f}$'nin veri deÄŸeri (data) olur. AynÄ± ÅŸekilde $\mathbf{f}$'nin gradyanÄ± $\frac{dL}{df}$, $\mathbf{d}$'nin veri deÄŸeri (data) olur.


![](./image/output3.png)


**KISACA**
UnutmayalÄ±m ki, $\mathbf{L = d \cdot f}$ iÅŸleminde biz **sadece bir sayÄ±nÄ±n** (mesela $d$'nin) deÄŸiÅŸtiÄŸini dÃ¼ÅŸÃ¼nÃ¼yoruz. DiÄŸer sayÄ± ($f$) o anlÄ±k iÃ§in **sabittir**.

#### 1. $\mathbf{L}$'nin $\mathbf{d}$'ye GÃ¶re TÃ¼revi ($\mathbf{\frac{\partial L}{\partial d}}$)

- **Basit MantÄ±k:** Diyelim ki $\mathbf{f}$'nin deÄŸeri $\mathbf{5}$ olsun. Ä°ÅŸlemimiz $\mathbf{L = d \cdot 5}$ haline gelir. $\mathbf{d}$'yi 1 birim artÄ±rÄ±rsam, $\mathbf{L}$'nin sonucu her zaman $\mathbf{5}$ kat artar.

- **Kural:** $\mathbf{5}$ (yani $\mathbf{f}$) bu iÅŸlemin **hÄ±z katsayÄ±sÄ±dÄ±r**. Bu yÃ¼zden $\mathbf{L}$'nin $\mathbf{d}$'ye gÃ¶re tÃ¼revi direkt olarak $\mathbf{f}$'nin kendisine eÅŸittir.
  
  $\mathbf{\frac{\partial L}{\partial d} = f}$

#### 2. $\mathbf{L}$'nin $\mathbf{f}$'ye GÃ¶re TÃ¼revi ($\mathbf{\frac{\partial L}{\partial f}}$)

- **Simetrik MantÄ±k:** Diyelim ki $\mathbf{d}$'nin deÄŸeri $\mathbf{4}$ olsun. Ä°ÅŸlemimiz $\mathbf{L = 4 \cdot f}$ haline gelir. $\mathbf{f}$'yi 1 birim artÄ±rÄ±rsam, $\mathbf{L}$'nin sonucu her zaman $\mathbf{4}$ kat artar.

- **Kural:** $\mathbf{4}$ (yani $\mathbf{d}$) bu iÅŸlemin **hÄ±z katsayÄ±sÄ±dÄ±r**. Bu nedenle, $\mathbf{L}$'nin $\mathbf{f}$'ye gÃ¶re tÃ¼revi direkt olarak $\mathbf{d}$'nin kendisine eÅŸittir:

$\mathbf{\frac{\partial L}{\partial f} = d}$

### 4. SorumluluÄŸu DaÄŸÄ±tma: $\mathbf{L}$'den $\mathbf{c}$ ve $\mathbf{e}$'ye Zincirleme

Bizim nihai amacÄ±mÄ±z, $\mathbf{L}$'deki hatayÄ±, $\mathbf{c}$ ve $\mathbf{e}$ gibi en baÅŸtaki girdilere kadar geri iletmektir. Bu, **Zincir KuralÄ± (Chain Rule)** ile yapÄ±lÄ±r.

$\mathbf{\frac{\partial L}{\partial c} = \frac{\partial L}{\partial d} \times \frac{\partial d}{\partial c}}$

Bu formÃ¼lde:

- $\mathbf{\frac{\partial L}{\partial d}}$ (Arka Gradyan): $\mathbf{L}$'den $\mathbf{d}$'ye gelen hatadÄ±r (Ã¶nceki adÄ±mda hesaplanmÄ±ÅŸtÄ±r).

- $\mathbf{\frac{\partial d}{\partial c}}$ (Yerel TÃ¼rev): Åu an hesapladÄ±ÄŸÄ±mÄ±z yerel etkidir.

**A)** $\mathbf{d}$'nin $\mathbf{c}$'ye GÃ¶re Yerel TÃ¼revi ($\mathbf{\frac{\partial d}{\partial c}}$)

$\mathbf{d = e + c}$ toplama iÅŸlemi iÃ§in $\mathbf{c}$'ye gÃ¶re yerel tÃ¼revi buluyoruz.``

| **AdÄ±m**             | **Ä°ÅŸlem (Limit TanÄ±mÄ±)**          | **AÃ§Ä±klama (Toplama Ä°ÅŸlemi iÃ§in Uygulama)**                                                                                                      |
| -------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| **BaÅŸlangÄ±Ã§**        | $\frac{f(x+h) - f(x)}{h}$ formÃ¼lÃ¼ | **Genel Kural:** DeÄŸiÅŸim hÄ±zÄ±nÄ± bulmak iÃ§in $\mathbf{x}$ yerine $\mathbf{c}$'yi, $\mathbf{f(x)}$ yerine $\mathbf{d}$'yi koyuyoruz.               |
| **DeÄŸiÅŸim Uygulama** | $\frac{(e + (c+h)) - (e + c)}{h}$ | **Kurala Uyum:** $\mathbf{c}$'yi $\mathbf{h}$ kadar artÄ±rÄ±p Ã§Ä±kan yeni $\mathbf{d}$ sonucunu hesapladÄ±k ve eski sonuÃ§tan Ã§Ä±kardÄ±k.               |
| **Parantezi AÃ§ma**   | $\frac{e + c + h - e - c}{h}$     | Pay kÄ±smÄ±nda terimlerin birbirini gÃ¶tÃ¼receÄŸini gÃ¶rme aÅŸamasÄ±.                                                                                    |
| **SadeleÅŸtirme**     | $\frac{h}{h}$                     | $\mathbf{e}$ ve $\mathbf{c}$ terimleri birbirini sÄ±fÄ±rladÄ±. Geriye sadece **saf $\mathbf{h}$ deÄŸiÅŸimi** kaldÄ±.                                   |
| **Nihai SonuÃ§**      | $\mathbf{1}$                      | $\mathbf{h}$'ler sadeleÅŸti. Kalan $\mathbf{1}$, $\mathbf{c}$'deki deÄŸiÅŸimin $\mathbf{d}$'yi **birebir** etkilediÄŸini gÃ¶steren hÄ±z katsayÄ±mÄ±zdÄ±r. |

**B)** $\mathbf{d}$'nin $\mathbf{e}$'ye GÃ¶re Yerel TÃ¼revi ($\mathbf{\frac{\partial d}{\partial e}}$)

AynÄ± iÅŸlemi $\mathbf{e}$ iÃ§in uygularsak, sonuÃ§ yine aynÄ± olur.

$\mathbf{\frac{\partial d}{\partial e} = 1}$

**C)** Nihai Gradyan HesaplamasÄ± ($\mathbf{L}$'nin $\mathbf{c}$ ve $\mathbf{e}$'ye GÃ¶re TÃ¼revi)

Åimdi elimizdeki bilgileri Zincir KuralÄ±'nda birleÅŸtirerek $\mathbf{c}$ ve $\mathbf{e}$'nin nihai gradyanÄ±nÄ± buluyoruz.

**Verilen DeÄŸerler (Ã–rnek Grafik Ã‡Ä±ktÄ±sÄ±ndan):**

- $\mathbf{\frac{\partial L}{\partial d}}$ (Arka Gradyan) = $\mathbf{-2}$ (Bu deÄŸer, $\mathbf{L=d \cdot f}$ iÅŸleminden $\mathbf{f}$'nin deÄŸeri olan $-2$'dir.)

- Yerel TÃ¼revler = $\mathbf{1}$

**I.** $\mathbf{c}$'nin Nihai GradyanÄ± ($\mathbf{\frac{\partial L}{\partial c}}$)

$\mathbf{\frac{\partial L}{\partial c} = \underbrace{\frac{\partial L}{\partial d}}_{\text{Arka Gradyan}} \times \underbrace{\frac{\partial d}{\partial c}}_{\text{Yerel TÃ¼rev}}}$

$\mathbf{\frac{\partial L}{\partial c} = (-2) \times (1) = -2}$

- **Ã‡Ä±karÄ±m:** $\mathbf{c}$'nin hataya olan nihai sorumluluÄŸu **$-2$**'dir.

** II.** $\mathbf{e}$'nin Nihai GradyanÄ± ($\mathbf{\frac{\partial L}{\partial e}}$)

$\mathbf{\frac{\partial L}{\partial e} = \underbrace{\frac{\partial L}{\partial d}}_{\text{Arka Gradyan}} \times \underbrace{\frac{\partial d}{\partial e}}_{\text{Yerel TÃ¼rev}}}$

$\mathbf{\frac{\partial L}{\partial e} = (-2) \times (1) = -2}$

- **Ã‡Ä±karÄ±m:** $\mathbf{e}$'nin hataya olan nihai sorumluluÄŸu da **$-2$**'dir.

**Ã–zetle:** Bir toplama iÅŸleminde, $\mathbf{d}$'den geriye gelen hata sinyali ($-2$), iki girdiye ($\mathbf{c}$ ve $\mathbf{e}$) **deÄŸiÅŸmeden, eÅŸit olarak** daÄŸÄ±tÄ±lÄ±r.


![](./image/output4.png)


### 5. SorumluluÄŸu DaÄŸÄ±tma: $\mathbf{e}$'den $\mathbf{a}$ ve $\mathbf{b}$'ye GeÃ§iÅŸ (Zincirin Sonu)

Bu son adÄ±mda, $\mathbf{e}$'ye ulaÅŸan hatayÄ± ($\frac{\partial L}{\partial e} = -2$) alÄ±p, $\mathbf{e}$'yi oluÅŸturan en baÅŸtaki girdilere ($\mathbf{a}$ ve $\mathbf{b}$) daÄŸÄ±tmalÄ±yÄ±z.

**A)** $\mathbf{e}$'nin $\mathbf{a}$'ya GÃ¶re Yerel TÃ¼revi ($\mathbf{\frac{\partial e}{\partial a}}$)

$\mathbf{e}$'nin oluÅŸumu da bir **Ã§arpma iÅŸlemidir**: $\mathbf{e = a \cdot b}$.

| **AdÄ±m**             | **Ä°ÅŸlem (Limit TanÄ±mÄ±)**                      | **AÃ§Ä±klama (Ã‡arpma Ä°ÅŸlemi iÃ§in Uygulama)**                                                                                                                         |
| -------------------- | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **BaÅŸlangÄ±Ã§**        | $\frac{f(x+h) - f(x)}{h}$ formÃ¼lÃ¼             | **Genel Kural:** DeÄŸiÅŸim hÄ±zÄ±nÄ± bulmak iÃ§in $\mathbf{x}$ yerine $\mathbf{a}$'yÄ±, $\mathbf{f(x)}$ yerine $\mathbf{e}$'yi koyuyoruz.                                 |
| **DeÄŸiÅŸim Uygulama** | $\frac{(a+h) \cdot b - a \cdot b}{h}$         | **Kurala Uyum:** $\mathbf{a}$'yÄ± $\mathbf{h}$ kadar artÄ±rÄ±p Ã§Ä±kan yeni $\mathbf{e}$ sonucunu hesapladÄ±k ve eski sonuÃ§tan Ã§Ä±kardÄ±k.                                 |
| **Parantezi AÃ§ma**   | $\frac{a \cdot b + h \cdot b - a \cdot b}{h}$ | Ã‡arpmayÄ± daÄŸÄ±ttÄ±k. $\mathbf{a \cdot b}$ terimlerinin birbirini gÃ¶tÃ¼receÄŸini gÃ¶rme aÅŸamasÄ±.                                                                         |
| **SadeleÅŸtirme**     | $\frac{h \cdot b}{h}$                         | $\mathbf{a \cdot b}$ terimleri birbirini sÄ±fÄ±rladÄ±. Geriye, $\mathbf{h}$ deÄŸiÅŸikliÄŸinden kaynaklanan **saf deÄŸiÅŸim** kaldÄ±.                                        |
| **Nihai SonuÃ§**      | $\mathbf{b}$                                  | $\mathbf{h}$'ler sadeleÅŸti. Kalan $\mathbf{b}$, $\mathbf{a}$'daki deÄŸiÅŸimin $\mathbf{e}$'yi tam olarak $\mathbf{b}$ kadar etkilediÄŸini gÃ¶steren hÄ±z katsayÄ±mÄ±zdÄ±r. |

$\mathbf{\frac{\partial e}{\partial a} = b}$

**B.** $\mathbf{e}$'nin $\mathbf{b}$'ye GÃ¶re TÃ¼revi

AynÄ± iÅŸlemi $\mathbf{b}$ iÃ§in uygularsak, sonuÃ§ yine aynÄ± olur.

$\mathbf{\frac{\partial e}{\partial b} = a}$

**C.** Nihai Gradyan HesaplamasÄ± ve Geri YayÄ±lÄ±mÄ±n Sonu

Åimdi $\mathbf{e}$'den gelen arka gradyanÄ± ($\mathbf{-2}$) alÄ±p, Zincir KuralÄ± ile son dÃ¼ÄŸÃ¼mlere ($\mathbf{a}$ ve $\mathbf{b}$) iletiyoruz.

**Verilen DeÄŸerler (Ã–rnek Grafik Ã‡Ä±ktÄ±sÄ±ndan):**

- $\mathbf{\frac{\partial L}{\partial e}}$ (Arka Gradyan) = $\mathbf{-2}$

- $\mathbf{a}$'nÄ±n Veri DeÄŸeri: $\mathbf{2.0}$

- $\mathbf{b}$'nin Veri DeÄŸeri: $\mathbf{-3.0}$

#### I. $\mathbf{a}$'nÄ±n Nihai GradyanÄ± ($\mathbf{\frac{\partial L}{\partial a}}$)

$\mathbf{\frac{\partial L}{\partial a} = \underbrace{\frac{\partial L}{\partial e}}_{\text{Arka Gradyan}} \times \underbrace{\frac{\partial e}{\partial a}}_{\text{Yerel TÃ¼rev (b)}}}$

$\mathbf{\frac{\partial L}{\partial a} = (-2) \times (-3.0) = \mathbf{6.0}}$

- **Ã‡Ä±karÄ±m:** $\mathbf{a}$'nÄ±n gradyanÄ± $\mathbf{6.0}$ olur.

#### II. $\mathbf{b}$'nin Nihai GradyanÄ± ($\mathbf{\frac{\partial L}{\partial b}}$)

$\mathbf{\frac{\partial L}{\partial b} = \underbrace{\frac{\partial L}{\partial e}}_{\text{Arka Gradyan}} \times \underbrace{\frac{\partial e}{\partial b}}_{\text{Yerel TÃ¼rev (a)}}}$

$\mathbf{\frac{\partial L}{\partial b} = (-2) \times (2.0) = \mathbf{-4.0}}$

- **Ã‡Ä±karÄ±m:** $\mathbf{b}$'nin gradyanÄ± $\mathbf{-4.0}$ olur.

**Geri YayÄ±lÄ±m Bitti:** TÃ¼m gradyanlar hesaplandÄ±! Sinir aÄŸÄ± artÄ±k bu deÄŸerleri ($\mathbf{6.0}$, $\mathbf{-4.0}$, $-2$ vb.) kullanarak aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncelleyebilir.


![](./image/output5.png)


Harika! TÃ¼m o tÃ¼rev hesaplamalarÄ±nÄ± ve zincir kuralÄ±nÄ± neden yaptÄ±ÄŸÄ±mÄ±zÄ±, yani Geri YayÄ±lÄ±mÄ±n (Backpropagation) sinir aÄŸlarÄ±nda ne anlama geldiÄŸini ÅŸimdi Ã¶zetliyoruz.

### 6. Geri YayÄ±lÄ±mÄ±n Sonu: Ã–ÄŸrenme BaÅŸlÄ±yor

Åimdiye kadar yaptÄ±ÄŸÄ±mÄ±z ÅŸey, bir sinir aÄŸÄ±nÄ±n yaptÄ±ÄŸÄ± **hata iÃ§in bir suÃ§ haritasÄ± Ã§Ä±karmaktÄ±.** O karmaÅŸÄ±k Zincir KuralÄ± ve tablolarla bulduÄŸumuz $\mathbf{6.0}$, $\mathbf{-4.0}$ veya $\mathbf{f}$ gibi tÃ¼m sayÄ±lar, bu haritanÄ±n Ã¼zerindeki **gradyanlardÄ±r (eÄŸimlerdir)**.

#### 1. Bu DeÄŸerleri Neden Bulduk? (AmacÄ±mÄ±z)

BulduÄŸumuz her bir gradyan deÄŸeri (Ã¶rneÄŸin $\mathbf{\frac{\partial L}{\partial a}} = \mathbf{+6.0}$), $\mathbf{L}$ (Hata/Loss) fonksiyonunun, $\mathbf{a}$ aÄŸÄ±rlÄ±ÄŸÄ±na gÃ¶re ne kadar hÄ±zlÄ± ve hangi yÃ¶nde deÄŸiÅŸtiÄŸini gÃ¶steren net bir Ã¶lÃ§Ã¼mdÃ¼r. Bu, bir pusula gÃ¶revi gÃ¶rÃ¼r; eÄŸer gradyan pozitif ve yÃ¼ksekse (Ã¶r:$\mathbf{+6.0}$), bu, $\mathbf{a}$ aÄŸÄ±rlÄ±ÄŸÄ±nÄ± artÄ±rmamÄ±z durumunda hatanÄ±n Ã§ok hÄ±zlÄ± artacaÄŸÄ± anlamÄ±na gelir. DolayÄ±sÄ±yla, hatayÄ± azaltmak iÃ§in, aÄŸÄ±rlÄ±ÄŸÄ± gradyanÄ±n gÃ¶sterdiÄŸi yÃ¶nÃ¼n **tam tersine**, yani **azaltma** yÃ¶nÃ¼ne doÄŸru hareket ettirmemiz gerektiÄŸini anlarÄ±z.

#### 2. Bu DeÄŸerlere Ne YapÄ±lacak? (Gradient Descent)

Bulunan gradyanlar **hemen kullanÄ±lÄ±r**. Bu, sinir aÄŸlarÄ±nÄ±n Ã¶ÄŸrenme kuralÄ± olan **Gradyan Ä°niÅŸi (Gradient Descent)** adÄ±mÄ±dÄ±r:

$\mathbf{\text{Yeni AÄŸÄ±rlÄ±k} = \text{Eski AÄŸÄ±rlÄ±k} - (\text{Ã–ÄŸrenme OranÄ±} \times \text{Gradyan})}$

- **Ã–ÄŸrenme OranÄ±**, aÄŸÄ±n her adÄ±mda ne kadar **kÃ¼Ã§Ã¼k** veya **bÃ¼yÃ¼k** bir dÃ¼zeltme yapacaÄŸÄ±nÄ± belirlerken; gradyanÄ±n yÃ¶nÃ¼, o aÄŸÄ±rlÄ±ÄŸÄ±n hatayÄ± azaltmak iÃ§in **artÄ±rÄ±lmasÄ±** mÄ± yoksa **azaltÄ±lmasÄ±** mÄ± gerektiÄŸini sÃ¶yler.

**KÄ±saca:** Geri YayÄ±lÄ±m ile bulduÄŸumuz gradyanlar, hemen bu formÃ¼le girerek **aÄŸÄ±rlÄ±klarÄ±n ayarlanmasÄ±nÄ± saÄŸlar**.

#### 3. Loss Fonksiyonu GerÃ§ekte NasÄ±l KullanÄ±lÄ±r?

Bizim Ã¶rneÄŸimizde sadece $L$ ile Ã§alÄ±ÅŸtÄ±k, ama gerÃ§ekte sinir aÄŸÄ± eÄŸitimi ÅŸÃ¶yledir:

1. **Tahmin:** AÄŸ bir Ã§Ä±ktÄ± ($y_{tahmin}$) Ã¼retir.

2. **Hata Hesaplama (Loss Function):** Bu Ã§Ä±ktÄ± ile doÄŸru cevap ($y_{gerÃ§ek}$) arasÄ±ndaki fark, genellikle **KayÄ±p Fonksiyonu (Loss Function)** ile Ã¶lÃ§Ã¼lÃ¼r. (Ã–rn: $L = (y_{tahmin} - y_{gerÃ§ek})^2$).

3. **Geri YayÄ±lÄ±m:** Geri YayÄ±lÄ±m, $\mathbf{L}$'den baÅŸlayarak hatayÄ± geriye yayar.

4. **AÄŸÄ±rlÄ±k GÃ¼ncelleme:** Gradyanlar kullanÄ±larak aÄŸÄ±rlÄ±klar hatayÄ± azaltacak ÅŸekilde ayarlanÄ±r.

Bu dÃ¶ngÃ¼ binlerce kez tekrarlandÄ±kÃ§a, aÄŸÄ±n aÄŸÄ±rlÄ±klarÄ± en uygun hale gelir ve hata minimuma iner.

### Ã–zet

Geri YayÄ±lÄ±mÄ± bitirerek elde ettiÄŸimiz tÃ¼m $\mathbf{grad}$ deÄŸerleri, sinir aÄŸÄ±mÄ±zÄ±n **bir sonraki denemede daha akÄ±llÄ± olmasÄ±nÄ± saÄŸlayan komutlardÄ±r.** Bu deÄŸerler, Gradyan Ä°niÅŸi formÃ¼lÃ¼ ile aÄŸÄ±rlÄ±klara hemen etki eder ve aÄŸÄ±n Ã¶ÄŸrenme dÃ¶ngÃ¼sÃ¼nÃ¼ tamamlar.

---

# 2. Ã–rnek: Manuel Geri YayÄ±lÄ±m


![2. Ã–rnek YapÄ±sÄ±](./image/example2_structure.png)


Åimdi yapÄ±ya gÃ¶re ikinci bir manuel geri yayÄ±lÄ±m yapacaÄŸÄ±z. KÄ±saca yapacaÄŸÄ±mÄ±z ÅŸey:

**Girdiler ve AÄŸÄ±rlÄ±klar:** $x1$ ve $x2$ girdilerimiz olacak ve $w1$ ve $w2$ aÄŸÄ±rlÄ±klarÄ±mÄ±z olacak bu girdiler iÃ§in. Bu, hÃ¼cre gÃ¶vdesinin solunda kalan kÄ±sÄ±m.

**HÃ¼cre GÃ¶vdesi:** YapmamÄ±z gereken gelen girdileri aÄŸÄ±rlÄ±klarla Ã§arpÄ±p toplayarak toplama bias ($b$) eklemek. KÄ±saca $x1$ ve $x2$ girdisi iÃ§in:

$$
(x_1 \cdot w_1 + x_2 \cdot w_2) + b
$$

ArdÄ±ndan bu Ã§Ä±ktÄ±yÄ± aktivasyon fonksiyonuna ekleyeceÄŸiz:

$$
\tanh((x_1 \cdot w_1 + x_2 \cdot w_2) + b)
$$

Bu bizim genel ileri besleme sistemimiz olacak. Åimdi bu adÄ±mdan sonra geri yayÄ±lÄ±ma bakalÄ±m.

---

## 1. AdÄ±m: BaÅŸlangÄ±Ã§ Durumu


![Ä°lk AÅŸama - TÃ¼m Gradyanlar 0](./image/output6.png)


Ä°lk aÅŸamada tÃ¼m gradyanlar 0 olur.

Åimdi son olan gradyanÄ± bulalÄ±m. En Ã¼st kÄ±sÄ±mda anlattÄ±ÄŸÄ±mÄ±z iÃ§in, bir deÄŸerin kendine gÃ¶re tÃ¼revi her zaman birdir, yani $\frac{do}{do} = 1$ olur. O yÃ¼zden $o$'nun tÃ¼revi $1$ olur.


![o'nun TÃ¼revi](./image/output7.png)


## 2. AdÄ±m: $n$'nin TÃ¼revini Bulma

$n$'nin tÃ¼revi iÃ§in $o$'nun $n$'ye gÃ¶re tÃ¼revine bakmamÄ±z lazÄ±m ki bu da Ã¼st kÄ±sÄ±mda yaptÄ±ÄŸÄ±mÄ±z gibi:

$$
\displaystyle \frac{e^{2x}-1}{e^{2x}+1}
$$

$\frac{f(x+h) - f(x)}{h}$ formÃ¼le girdi olarak verebiliriz:

$$
\displaystyle f(x) = \frac{e^{2x}-1}{e^{2x}+1}
$$

Fonksiyonunu limit tanÄ±mÄ±yla tÃ¼rev adÄ±mlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼relim. Tablo ÅŸÃ¶yle olur:

| **AdÄ±m**                           | **Ä°ÅŸlem (Limit TanÄ±mÄ± / AÃ§Ä±klama)**                                                                                                                              | **AÃ§Ä±klama (tanh iÃ§in uygulama)**                                                                                                                     |
| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **BaÅŸlangÄ±Ã§**                      | $\frac{f(x+h) - f(x)}{h}$                                                                                                                                        | **Genel Kural:** TÃ¼revi bulmak iÃ§in limit tanÄ±mÄ±nÄ± kullanÄ±yoruz. Burada $f(x) = \tanh(x) = \frac{e^{2x}-1}{e^{2x}+1}$                                 |
| **DeÄŸiÅŸim Uygulama**               | $\frac{\frac{e^{2(x+h)}-1}{e^{2(x+h)}+1} - \frac{e^{2x}-1}{e^{2x}+1}}{h}$                                                                                        | (x) deÄŸerini (x+h) ile deÄŸiÅŸtirdiÄŸimizde Ã§Ä±kan yeni fonksiyon deÄŸerini eski deÄŸerden Ã§Ä±karÄ±yoruz.                                                     |
| **Payda BirleÅŸtirme**              | $\frac{(e^{2(x+h)}-1)(e^{2x}+1) - (e^{2x}-1)(e^{2(x+h)}+1)}{h \cdot (e^{2(x+h)}+1)(e^{2x}+1)}$                                                                   | Ä°ki kesiri tek paydada birleÅŸtirdik, limit formÃ¼lÃ¼nÃ¼ uygulamak iÃ§in.                                                                                  |
| **Parantezi AÃ§ma**                 | $\frac{e^{2(x+h)+2x}+e^{2(x+h)} - e^{2x}-1 - e^{2x+2(x+h)} - e^{2x} + e^{2(x+h)} +1}{h \cdot (e^{2(x+h)}+1)(e^{2x}+1)}$                                          | Ã‡arpÄ±mlarÄ± daÄŸÄ±tarak terimleri sadeleÅŸtirmeye hazÄ±r hale getiriyoruz.                                                                                 |
| **SadeleÅŸtirme**                   | $\frac{2h \cdot e^{2x} + \mathcal{O}(h^2)}{h \cdot (e^{2x}+1)^2}$                                                                                                | $Limit (h \to 0) $alÄ±nÄ±rken sadece **h ile orantÄ±lÄ± terimler** kalÄ±r, yÃ¼ksek mertebeden kÃ¼Ã§Ã¼k terimler ihmal edilir.                                  |
| **Nihai SonuÃ§ (Exponential Form)** | $\displaystyle \frac{2 e^{2x}}{(e^{2x}+1)^2}$                                                                                                                    | TÃ¼revin **exponential formu**.                                                                                                                        |
| **Nihai SonuÃ§ ($\tanh$ Formu)**    | $\displaystyle 1 - \tanh^2(x) = 1 - \left(\frac{e^{2x}-1}{e^{2x}+1}\right)^2 = \frac{(e^{2x}+1)^2 - (e^{2x}-1)^2}{(e^{2x}+1)^2} = \frac{2 e^{2x}}{(e^{2x}+1)^2}$ | Exponential formu, $\tanh$ tanÄ±mÄ±nÄ± kullanarak sadeleÅŸtirildi. BÃ¶ylece $\displaystyle \frac{2 e^{2x}}{(e^{2x}+1)^2} = 1 - \tanh^2(x)$ olarak yazÄ±lÄ±r. |

---

ğŸ’¡ **Not:** Son adÄ±mda, $\displaystyle \frac{2 e^{2x}}{(e^{2x}+1)^2}$ ifadesi $1 - \tanh^2(x)$ ile eÅŸdeÄŸerdir.

Ya da daha kÄ±saca $\tanh$'Ä±n tÃ¼revine internetten bakacak olursak, Ã¶rneÄŸin Vikipedi'de $\tanh$'Ä±n tÃ¼revi aÅŸaÄŸÄ±daki gibidir:

$$
\displaystyle \frac{d}{dx} \tanh x = 1 - \tanh^2 x = \operatorname{sech}^2 x = \frac{1}{\cosh^2 x}
$$

FormÃ¼lÃ¼nden direkt $1 - \tanh^2(x)$ bunu biliriz. Biz burada $\tanh()$ deÄŸerini biliyoruz:

$$
\displaystyle \frac{do}{dn} = 1 - \tanh(n)^2
$$

Biz $\tanh(n)$'in $o$ olduÄŸunu biliyoruz, o zaman:

$$
1 - o^2
$$

Bu da grafikten $o.\text{data} = 0.7071$ dir ve $1 - 0.7071^2 = 0.5$ olur.

O zaman $n$'nin gradyanÄ± $0.5$ olur.


![n'nin GradyanÄ±](./image/output8.png)


# 3. SorumluluÄŸu DaÄŸÄ±tma: (o)â€™dan (x1w1x2w2) ve (b)â€™ye Zincirleme

Bizim nihai amacÄ±mÄ±z, (o)â€™daki hatayÄ±, **x1w1x2w2** ve **b** gibi en baÅŸtaki girdilere kadar geri iletmektir. Bu, **Zincir KuralÄ± (Chain Rule)** ile yapÄ±lÄ±r.

$\frac{\partial o}{\partial x} = \frac{\partial o}{\partial n} \times \frac{\partial n}{\partial x} $

Burada:

- $\frac{\partial o}{\partial n}$ (Arka Gradyan): oâ€™dan nâ€™ye gelen hatadÄ±r (Ã¶nceki adÄ±mda tanh tÃ¼revinden hesaplanmÄ±ÅŸtÄ±r, Ã¶rn: 0.5).

- $\frac{\partial n}{\partial x}$ (Yerel TÃ¼rev): Åu an hesapladÄ±ÄŸÄ±mÄ±z yerel etkidir.

---

## **A)** (n)'nin (x1w1x2w2)'ye GÃ¶re Yerel TÃ¼revi $\frac{\partial n}{\partial x1w1x2w2}$

$n = x1w1x2w2 + b$

| **AdÄ±m**             | **Ä°ÅŸlem (Limit TanÄ±mÄ±)**                        | **AÃ§Ä±klama (Toplama Ä°ÅŸlemi iÃ§in Uygulama)**                                      |
| -------------------- | ----------------------------------------------- | -------------------------------------------------------------------------------- |
| **BaÅŸlangÄ±Ã§**        | $\frac{f(x+h) - f(x)}{h}$                       | Genel kural: deÄŸiÅŸim hÄ±zÄ±nÄ± bulmak iÃ§in $x \to x1w1x2w2$, $f(x) \to n$ koyuyoruz |
| **DeÄŸiÅŸim Uygulama** | $\frac{(x1w1x2w2 + h + b) - (x1w1x2w2 + b)}{h}$ | x1w1x2w2'yi h kadar artÄ±rÄ±p Ã§Ä±kan yeni n deÄŸerini eski n deÄŸerinden Ã§Ä±kardÄ±k     |
| **Parantezi AÃ§ma**   | $\frac{x1w1x2w2 + h + b - x1w1x2w2 - b}{h}$     | Terimleri aÃ§Ä±yoruz                                                               |
| **SadeleÅŸtirme**     | $\frac{h}{h}$                                   | b terimi birbirini gÃ¶tÃ¼rdÃ¼, geriye sadece h kaldÄ±                                |
| **Nihai SonuÃ§**      | 1                                               | x1w1x2w2'deki deÄŸiÅŸimin n'yi birebir etkilediÄŸini gÃ¶sterir                       |

---

## **B)** (n)'nin (b)'ye GÃ¶re Yerel TÃ¼revi $\frac{\partial n}{\partial b}$

AynÄ± iÅŸlemi b iÃ§in uygularsak, sonuÃ§ yine aynÄ± olur:

$\frac{\partial n}{\partial b} = 1$

---

## **C)** Nihai Gradyan HesaplamasÄ± ((o)'nun (x1w1x2w2) ve (b)'ye GÃ¶re TÃ¼revi)

Åimdi elimizdeki bilgileri Zincir KuralÄ±â€™nda birleÅŸtiriyoruz:

**Verilen DeÄŸerler (Ã–rnek):**

- $\frac{\partial o}{\partial n}$ (Arka Gradyan) = 0.5 (tanh tÃ¼revinden)

- Yerel tÃ¼revler = 1

---

**I.** (x1w1x2w2)â€™nin Nihai GradyanÄ±

$\frac{\partial o}{\partial x1w1x2w2} = \underbrace{\frac{\partial o}{\partial n}}*{\text{Arka Gradyan}} \times \underbrace{\frac{\partial n}{\partial x1w1x2w2}}*{\text{Yerel TÃ¼rev}} $

$\frac{\partial o}{\partial x1w1x2w2} = 0.5 \times 1 = 0.5 $

- **Ã‡Ä±karÄ±m:** x1w1x2w2â€™nin hataya olan nihai sorumluluÄŸu **0.5**â€™tir.

---

**II.** (b)â€™nin Nihai GradyanÄ±

$\frac{\partial o}{\partial b} = \underbrace{\frac{\partial o}{\partial n}}*{\text{Arka Gradyan}} \times \underbrace{\frac{\partial n}{\partial b}}*{\text{Yerel TÃ¼rev}} $

$\frac{\partial o}{\partial b} = 0.5 \times 1 = 0.5 $

- **Ã‡Ä±karÄ±m:** bâ€™nin hataya olan nihai sorumluluÄŸu da **0.5**â€™tir.

---

**Ã–zetle:** Bir toplama iÅŸleminde, **oâ€™dan geriye gelen hata sinyali (0.5)**, iki girdiye ((x1w1x2w2) ve b) **deÄŸiÅŸmeden, eÅŸit olarak** daÄŸÄ±tÄ±lÄ±r.


![](./image/output9.png)


## 4. AdÄ±m: SorumluluÄŸu DaÄŸÄ±tma - $o$'dan $x1w1$ ve $x2w2$'ye Zincirleme

YapÄ±, 3. aÅŸamadaki **n = x1w1x2w2 + b** mantÄ±ÄŸÄ±yla tamamen aynÄ±dÄ±r. Bu nedenle **tÃ¼m aÅŸamalar aynÄ± ÅŸekilde uygulanÄ±r ve sonuÃ§lar da aynÄ± olur**. Yani 3. aÅŸamada nâ€™deki gradyanlar x1w1x2w2 ve bâ€™ye **aynÄ± ÅŸekilde aktarÄ±lmÄ±ÅŸtÄ±**.

Zincir kuralÄ±ndan:

$\frac{\partial o}{\partial x} = \frac{\partial o}{\partial n} \times \frac{\partial n}{\partial x}$

Burada $\frac{\partial n}{\partial x1w1} = \frac{\partial n}{\partial x2w2} = 1$ olduÄŸu iÃ§in gradyanlar 0.5 olarak aktarÄ±lÄ±r.

* Bu nedenle, **x1w1 ve x2w2â€™ye de 0.5 deÄŸerleri zincirleme ile aktarÄ±lÄ±r**.

---

### **SonuÃ§lar (Gradyanlar)**

$\frac{\partial o}{\partial x1w1} = 0.5$

$\frac{\partial o}{\partial x2w2} = 0.5$

$\frac{\partial o}{\partial x1w1x2w2} = 0.5 \quad (\text{3. aÅŸamadan})$

**Not:** Toplama iÅŸlemi olduÄŸu iÃ§in x1w1x2w2 deki grad her parÃ§a (x1w1 ve x2w2) gradyanÄ± zincirleme mantÄ±ÄŸÄ± gereÄŸi**eÅŸit ÅŸekilde** alÄ±r.

## 5. AdÄ±m: SorumluluÄŸu DaÄŸÄ±tma - $x1w1$ ve $x2w2$'den $x_1, w_1, x_2, w_2$'ye GeÃ§iÅŸ

Bu son adÄ±mda, $x1w1$ ve $x2w2$'ye ulaÅŸan hatayÄ± alÄ±p, bunlarÄ± oluÅŸturan en baÅŸtaki girdilere ($x_1, w_1, x_2, w_2$) daÄŸÄ±tmalÄ±yÄ±z.

---

| AdÄ±m         | Ä°ÅŸlem (Limit TanÄ±mÄ±)                               | AÃ§Ä±klama (Ã‡arpma Ä°ÅŸlemi UygulamasÄ±)                                    |
| ------------ | -------------------------------------------------- | ---------------------------------------------------------------------- |
| BaÅŸlangÄ±Ã§    | $\frac{f(x+h)-f(x)}{h}$                            | DeÄŸiÅŸim hÄ±zÄ±nÄ± bulmak iÃ§in x yerine x1â€™i, f(x) yerine x1w1â€™i koyuyoruz |
| DeÄŸiÅŸim      | $\frac{(x1+h) \cdot w1 - x1 \cdot w1}{h}$          | x1â€™i h kadar artÄ±rÄ±yoruz                                               |
| AÃ§ma         | $\frac{x1 \cdot w1 + h \cdot w1 - x1 \cdot w1}{h}$ | Terimleri aÃ§Ä±yoruz                                                     |
| SadeleÅŸtirme | $\frac{h \cdot w1}{h}$                             | Geriye sadece hâ€™den kaynaklÄ± deÄŸiÅŸim kaldÄ±                             |
| Nihai SonuÃ§  | $w1$                                               | x1â€™deki deÄŸiÅŸimin x1w1â€™i w1 kadar etkilediÄŸini gÃ¶sterir                |

$\frac{\partial x1w1}{\partial x1} = w1, \quad \frac{\partial x1w1}{\partial w1} = x1$

> **Not:** Burada Ã§arpma iÅŸleminin simetrik etkisi var; x1â€™deki deÄŸiÅŸim w1 kadar, w1â€™deki deÄŸiÅŸim x1 kadar etki yapÄ±yor.

---

### B) x2w2â€™nin x2â€™ye ve w2â€™ye gÃ¶re yerel tÃ¼revleri

x2w2 = x2 * w2

$\frac{\partial x2w2}{\partial x2} = w2, \quad \frac{\partial x2w2}{\partial w2} = x2$

> **Not:** AynÄ± mantÄ±k x2w2 iÃ§in de geÃ§erli. Ã‡arpma iÅŸlemi olduÄŸu iÃ§in, zincir kuralÄ±yla nihai gradyanlar da Ã§arpÄ±larak hesaplanÄ±r.

---

### C) Nihai Gradyan HesaplamasÄ± (Geri YayÄ±lÄ±m)

x1w1 ve x2w2â€™den gelen arka gradyanlar:

$\frac{\partial o}{\partial x1w1} = 0.5, \quad \frac{\partial o}{\partial x2w2} = 0.5$

#### I. x1 ve w1

$\frac{\partial o}{\partial x1} = \frac{\partial o}{\partial x1w1} \cdot \frac{\partial x1w1}{\partial x1} = 0.5 * (-3.0) = -1.5$

$\frac{\partial o}{\partial w1} = \frac{\partial o}{\partial x1w1} \cdot \frac{\partial x1w1}{\partial w1} = 0.5 * 2.0 = 1.0$

* **Ã‡Ä±karÄ±m:** x1â€™in gradyanÄ± -1.5, w1â€™in gradyanÄ± 1.0 olur.

#### II. x2 ve w2

$\frac{\partial o}{\partial x2} = \frac{\partial o}{\partial x2w2} \cdot \frac{\partial x2w2}{\partial x2} = 0.5 * 1.0 = 0.5$

$\frac{\partial o}{\partial w2} = \frac{\partial o}{\partial x2w2} \cdot \frac{\partial x2w2}{\partial w2} = 0.5 * 0.0 = 0.0$

* **Ã‡Ä±karÄ±m:** x2â€™nin gradyanÄ± 0.5, w2â€™nin gradyanÄ± 0.0 olur.

> **Not:** Burada w2 = 0 olduÄŸu iÃ§in gradyan da 0 oldu; yani bu aÄŸÄ±rlÄ±k hataya katkÄ±da bulunmuyor.


![](./image/output10.png)


Elbette! TÃ¼m bu teknik akÄ±ÅŸÄ±, notlarÄ±nÄ±za ekleyebileceÄŸiniz resmi, sade ve Ã¶z bir dille Ã¶zetliyorum.

---

### ğŸ“ Geri YayÄ±lÄ±mÄ±n Ã–zeti ve Otomasyona GeÃ§iÅŸ

Bu aÅŸama, sinir aÄŸÄ±nÄ±n Ã¶ÄŸrenme dÃ¶ngÃ¼sÃ¼nÃ¼ tamamlar ve manuel hesaplamadan otomatik sisteme geÃ§iÅŸin gerekÃ§esini sunar.

#### I. Gradyan Ã‡Ä±karÄ±mÄ± ve $\mathbf{x_2 = 0}$ Durumu

Geri YayÄ±lÄ±m (Backpropagation) zincirleme kuralÄ±nÄ±n uygulanmasÄ±yla her bir aÄŸÄ±rlÄ±ÄŸÄ±n ($\mathbf{w}$) ve girdinin ($\mathbf{x}$) hataya olan **sorumluluÄŸu (gradyanÄ±)** bulunur.

- **SÄ±fÄ±r Gradyan:** Hesaplama sÄ±rasÄ±nda $\mathbf{x_2}$ girdisinin deÄŸerinin $\mathbf{0}$ olmasÄ± nedeniyle, Ã§arpÄ±m iÅŸleminden sonra $\mathbf{w_2}$ aÄŸÄ±rlÄ±ÄŸÄ±nÄ±n gradyanÄ± da $\mathbf{0}$ Ã§Ä±kar. Bunun sezgisel anlamÄ± ÅŸudur: $\mathbf{w_2}$'yi deÄŸiÅŸtirmenin nihai Ã§Ä±ktÄ± Ã¼zerinde hiÃ§bir etkisi yoktur, Ã§Ã¼nkÃ¼ $\mathbf{0}$ ile Ã§arpÄ±lmaktadÄ±r.
  
  Bu durumun eÄŸitimdeki sonucu ÅŸudur: **$\mathbf{w_2}$'nin gradyanÄ± sÄ±fÄ±r olduÄŸu iÃ§in, o aÄŸÄ±rlÄ±k Ã¼zerinden hatayÄ± azaltma Ã§abasÄ± imkansÄ±zdÄ±r; $\mathbf{w_2}$ gÃ¼ncellenmez.** Ä°ÅŸte bu noktada, sinir aÄŸÄ±nÄ±n Ã¶ÄŸrenme enerjisi (Geri YayÄ±lÄ±m) otomatik olarak $\mathbf{w_2}$'den uzaklaÅŸÄ±r ve hataya gerÃ§ekten katkÄ±da bulunan, yani gradyanÄ± sÄ±fÄ±rdan farklÄ± olan diÄŸer aÄŸÄ±rlÄ±klara (Ã¶rneÄŸin $\mathbf{w_1}$'e) yÃ¶nelir. Bu, gradyanlarÄ±n sinir aÄŸÄ±nda **"hangi parÃ§anÄ±n Ã¶nemli olduÄŸunu"** ve nerenin ayarlanmasÄ± gerektiÄŸini gÃ¶steren hayati bir pusula gÃ¶revi gÃ¶rdÃ¼ÄŸÃ¼nÃ¼ kanÄ±tlar.

#### II. Otomasyona GeÃ§iÅŸin GereÄŸi

- **Manuel SÃ¼reÃ§:** TÃ¼m bu gradyanlarÄ±n (Ã¶rneÄŸin $\mathbf{x_1.grad}, \mathbf{w_1.grad}$) elle hesaplanmasÄ±, gerÃ§ek sinir aÄŸlarÄ±ndaki milyonlarca parametre dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼nde **verimsiz ve pratik dÄ±ÅŸÄ±dÄ±r**. Ä°ÅŸte bu nedenle, **otomatik tÃ¼rev (autograd)** mekanizmasÄ±nÄ± kurmak zorundayÄ±z. `Value` sÄ±nÄ±fÄ±na, her bir iÅŸlemde (Ã§arpma, toplama vb.) o iÅŸlemin yerel tÃ¼revini hesaplayÄ±p geriye doÄŸru ileten Ã¶zel bir fonksiyon (`self._backward`) eklenir.
  
  Bu sistem, **Zincir KuralÄ±'nÄ±n kÃ¼Ã§Ã¼k bir parÃ§asÄ±nÄ±** her dÃ¼ÄŸÃ¼mde hafÄ±zaya kaydeder ve gerektiÄŸinde bu kurallarÄ± otomatik olarak uygulayarak gradyanlarÄ± saniyeler iÃ§inde hesaplar.
  
  ---

| AÅŸama                           | Ne Olur?                                                                                                                         | Ã–rnek                                  | Geri YayÄ±lÄ±mda RolÃ¼                                                                             |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------- | ----------------------------------------------------------------------------------------------- |
| 1. GiriÅŸ (Input)                | AÄŸ, Ã¶ÄŸrenmek istediÄŸi veriyi alÄ±r (sayÄ±sal girdiler, x'ler).                                                                     | 3â€“5 tane girdi                         | BaÅŸlangÄ±Ã§ noktasÄ±dÄ±r, gradyan en son buraya ulaÅŸÄ±r.                                             |
| 2. Ä°leri Besleme (Forward Pass) | Girdiler, aÄŸÄ±rlÄ±klarla Ã§arpÄ±lÄ±p toplanarak katmanlardan geÃ§er. Bu aÅŸamada zincir kurulur.                                        | (aâ‹…b + c) gibi iÅŸlemler                | Zincirin kurulduÄŸu aÅŸamadÄ±r (_prev ve _op kaydedilir).                                          |
| 3. Ã‡Ä±ktÄ± (Output)               | AÄŸ bir tahmin (y_tahmin) Ã¼retir.                                                                                                 | Ã‡Ä±ktÄ± 4.8, kedi                        | Tahminin hatasÄ± Ã¶lÃ§Ã¼lÃ¼r.                                                                        |
| 4. Hata/KayÄ±p (Loss)            | Tahmin ile gerÃ§ek deÄŸer (y_gerÃ§ek) arasÄ±ndaki fark, KayÄ±p Fonksiyonu (L) ile Ã¶lÃ§Ã¼lÃ¼r.                                            | L = (y_tahmin âˆ’ y_gerÃ§ek)Â²             | Geri yayÄ±lÄ±mÄ±n baÅŸlangÄ±Ã§ noktasÄ±dÄ±r (L.grad = 1 ile baÅŸlar).                                    |
| 5. Geri YayÄ±lÄ±m (Backprop)      | L'den gelen hata sinyali, Zincir KuralÄ± ile tersten tÃ¼m aÄŸÄ±rlÄ±klara doÄŸru yayÄ±lÄ±r.                                               | âˆ‚a/âˆ‚L = 6.0 gibi gradyanlar hesaplanÄ±r | Sorumluluk daÄŸÄ±tÄ±lÄ±r; her aÄŸÄ±rlÄ±k, hataya ne kadar katkÄ±sÄ± olduÄŸunu Ã¶ÄŸrenir.                    |
| 6. GÃ¼ncelleme                   | Bulunan gradyanlar, Ã¶ÄŸrenme oranÄ± ile Ã§arpÄ±lÄ±r ve aÄŸÄ±rlÄ±klar hatayÄ± azaltacak ÅŸekilde ayarlanÄ±r (Gradient Descent).              | Yeni AÄŸÄ±rlÄ±k = Eski AÄŸÄ±rlÄ±k âˆ’ (â€¦)      | AÄŸ kendisini dÃ¼zelterek bir sonraki deneme iÃ§in hazÄ±rlanÄ±r.                                     |
| Ek Not                          | KayÄ±p deÄŸeri (L) hakkÄ±nda: L deÄŸeri, geri yayÄ±lÄ±m hesaplamasÄ±nda doÄŸrudan kullanÄ±lmaz, ancak eÄŸitim sÃ¼recinin ana gÃ¶stergesidir. | â€”                                      | L, aÄŸÄ±n performansÄ±nÄ± Ã¶lÃ§mek ve eÄŸitimin ne zaman durdurulacaÄŸÄ±na karar vermek iÃ§in kullanÄ±lÄ±r. |

--- 
